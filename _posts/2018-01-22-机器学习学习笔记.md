---
layout: post
title: '机器学习学习笔记'
date: 2018-01-22
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 深度学习 机器学习
---
> 网易云课堂-吴恩达机器学习视频知识整理

## 机器学习

定义：
He defined machine learning as the field of study that gives computers the ability to learn without being explicitly programmed
他将机器学习定义为在没有明确设置的情况下使计算机具有学习能力的研究领域
A computer program is said to learn from experience E with respect to some task T and some performance measure P , if its performance on T , as measured by P , improves with experience E.
 计算机程序从经验e 中学习，解决某一任务t 进行某一性能度量p 。通过p测定在t上的表现因经验e而提高

强化学习(Reinforcement learning)


选择直线进行拟合还是选择二次函数进行拟合  

## 监督学习
回归问题(regression problem) 是我们设法预测连续值的属性。线性回归模型（Linear Regression Model）
分类问题(classification problem) 是指预测离散值的属性。

## 无监督学习
聚类问题  没有标签，寻找数据相同的结构

## 代价函数（cost）
神经网络函数构建
![](/assets/img/tensorflow/data/machinelearning/1.jpg)

假设函数

代价函数

简化版代价函数 将θ(0)简化为0
![](/assets/img/tensorflow/data/machinelearning/2.jpg)
![](/assets/img/tensorflow/data/machinelearning/3.jpg)

参数增加将变成多维图形
![](/assets/img/tensorflow/data/machinelearning/4.jpg)
![](/assets/img/tensorflow/data/machinelearning/5.jpg)
寻找代价函数最小的点，意味着获取到拟合最好的函数

## 梯度下降（Gradient descent algorithm）(线性回归)

1、开始给定参数θ(0)θ(1)初始值，不关心具体的值，利用高斯初始化即可或者为0初始化。
2、改变θ(0)θ(1)的值，找到J(θ(0),θ(1))代价函数的最小值或者局部最小值
![](/assets/img/tensorflow/data/machinelearning/6.jpg)

数学原理
![](/assets/img/tensorflow/data/machinelearning/7.jpg)
其中α(学习速率)控制的是梯度下降时，我们迈出多大的步子，下降的比率。更新θ(j)。如果太小，速度会慢，太大会越过最低点或者无法收敛甚至发散。一般在调整时可以进行3倍的调整。主要可以根据梯度下降的（代价函数的变化）函数图像进行观察，或者进行自动判断收敛的方法，代价函数的变化值是否小于某个极小值10的-3次幂（但是这个极小值很难确定）。
![](/assets/img/tensorflow/data/machinelearning/12.jpg)
其中更新步骤需要同时更新（simultaneous update）
导数项，决定着θ(j)是增加还是减小
![](/assets/img/tensorflow/data/machinelearning/8.jpg)

局部最优点
如果在局部最优点，将不会改变

以上的cost代价函数加梯度下降，以整个训练集为每次的计算数据，叫做batch梯度下降法

## 矩阵（matrix）和向量(vector)

 矩阵 n*m 大写字母表示
 向量 n*1 小写字母表示  1-indexed  and 0-indexed
 求和运算 两个矩阵维度相等
 乘法运算
 标量*矩阵，对应位置相乘    满足交换率
 矩阵相乘
 单位矩阵  对角线为1
 逆矩阵    矩阵*矩阵的逆矩阵=矩阵的逆矩阵*矩阵=单位矩阵
 矩阵转置  1、沿对角线翻转。2、列变成行

## 多元线性回归
n 表示特征数
m 表示训练样本集数
多个特征值，假设函数形式变为  hθ(x) = θ(0)+θ(1)x(1)+θ(2)x(2)....+θ(n)x(n)
为了简化，会增加一个x(0)特征量，默认为1，hθ(x) 函数即表示为θ向量的转置*x向量
![](/assets/img/tensorflow/data/machinelearning/9.jpg)

## 多元梯度下降
![](/assets/img/tensorflow/data/machinelearning/10.jpg)

在多元梯度下降时，需要对于参数进行特征缩放（1、除最大值2、均值归一化：减均值除以值域或标准差），将特征的取值约束到-1到+1的范围内
![](/assets/img/tensorflow/data/machinelearning/11.jpg)

## 特征的选择
例如，当房屋价格的特征包含临街的宽度，与垂直宽度两个特征时，我们的假设函数为h=x0+θ1*x1+θ2*x2
但是做为宽度和高度其实是房屋的占地面积，如果将其合并成一个特征。可能对于该假设函数更适合h=x0+θ1*x

当特征选择平方、立方、根时 h=x0+θ1*x1+θ2*x1*x1  h=x0+θ1*x1+θ2*x1*x1+θ2*x1*x1*x1   假设函数为多项式函数

## 代价最小值

### 梯度下降
（适用当特征大于10000时，并且对于其他的算法也具有很好的支持）

### 正规方程(normal equation method)

求代价函数的偏导数为0时的θ的值。（适用当特征小于10000时，并且是线性回归的问题）
正规方程解法不需要进行特征缩放（features scaling）

![](/assets/img/tensorflow/data/machinelearning/15.jpg)

![正规方程解法](/assets/img/tensorflow/data/machinelearning/13.jpg)

![梯度下降与正规方程优缺点对比](/assets/img/tensorflow/data/machinelearning/14.jpg)

#### 正规方程不可逆性
1、如果矩阵呈线性关系，则该矩阵不可逆
2、训练集样本数太少，例如对于100个特征，10个训练数据则为少
不可逆的，要从特征入手，进行删节
![](/assets/img/tensorflow/data/machinelearning/16.jpg)
