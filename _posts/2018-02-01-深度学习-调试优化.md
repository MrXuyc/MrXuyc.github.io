---
layout: post
title: '深度学习-调优优化'
date: 2018-01-22
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 深度学习 深层神经网络
---
> 网易云课堂-吴恩达深度学习视频知识整理

## 深度学习的实用层面
### 数据集分布
数据根据作用区分为训练集（training set） 验证集（dev set） 测试集（test set）  
数据规模小 70/30   60/20/20  
数据规模大（100W）  98/1/1     
数据越大验证集和测试集所占比例越小。训练集 验证集 测试集要来自同一分布。  
如果不需要无偏评估算法性能 则不需要测试集，将验证集改称为测试集（训练验证集）。  

### 偏差/方差
#### 情况
用来研究多维度时欠拟合、适度拟合、过拟合的程度  
假设人区分错误率为0    最优误差为贝叶斯误差，接近0  
训练集错误率  1%   验证集错误率  11%  为高方差 有过度拟合风险  
训练集错误率  15%  验证集错误率  16%  为高偏差 有欠拟合风险  
训练集错误率  15%  验证集错误率  30%  为高方差高偏差 有欠拟合风险  
训练集错误率  0.5% 验证集错误率  1%  为低偏差低方差  
#### 处理方式
![](/assets/img/tensorflow/data/deeplearning/20.jpg)
高偏差  更改神经网络层数及节点数量  
高方差  增大数据、正则化  

### 正则化
#### 解决高方差(过度拟合)问题  
lambd做为超参数  
逻辑回归的正则化  
![](/assets/img/tensorflow/data/deeplearning/21.jpg)
L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1  
L1正则化会产生稀疏矩阵即有很多元素为0，只有少数元素为非零值的矩阵。会达到特征选择的效果。  
神经网络的 正则化   Frobenius范数  相当于w 需要乘上(1-αλ/m)的权重  
![](/assets/img/tensorflow/data/deeplearning/22.jpg)
L2正则化是指权值向量w中各个元素的欧几里德范数平方，通常表示为||w||2  
#### 为什么正则化对防止过拟合化有帮助？
将函数变得简单，减少部分w对结果的影响  
![](/assets/img/tensorflow/data/deeplearning/23.jpg)
将函数变得简单，让激活函数的值稳定在线性部分。  
![](/assets/img/tensorflow/data/deeplearning/24.jpg)
#### Dropout
随机关闭节点  
反向随机 inverted dropout采用除以keep-prob的方式确保a期望值不变  
参数较多的隐藏层可以蒋keep_prod调整较低0.5。  
#### early-stopping
提前停止训练模型  
![](/assets/img/tensorflow/data/deeplearning/25.jpg)
优势是计算简单  
弊端是当停止时，成本函数J可能还没达到最小值。  

### 归一化
1、零均值化  变量-均值  
2、归一化方差  
其中参数都是由训练集计算得出，直接用于验证集和测试集。  

### 梯度下降
#### 梯度消失/梯度爆炸
原因：如果激活函数使用线性的，并且忽略b值，则如果w大于1则会呈指数增长，小于1，如0.9则会指数减少。  
缓解方法：  
np.random.randn(shape) * np.sqrt(2/n[l-1]) 设置权重参数w  还可以调整方差参数  
relu (2/n[l-1])的开方   tanh  (1/n[l-1])的开方    其他 (2/n[l-1]+n[l])的开方  

#### 梯度下降检验
双边导数验证
![](/assets/img/tensorflow/data/deeplearning/26.jpg)

判断数值梯度和解析梯度值之间的差（双边导数值和J的导数/θ的导数），如果在10的-7次 属于非常好，在10的-3次则需要注意。
![](/assets/img/tensorflow/data/deeplearning/27.jpg)

验证时注意事项
1、不要在训练中使用梯度检验，它只用于调试。（因为速度太慢，只用于验证）
2、如果算法的梯度检验失败，要检查所有项。  
3、如果使用正则化，请注意正则项。  
4、梯度检验不能跟dropout同时使用。  
5、当w和b接近0时，梯度下降的实施是正确的，在随机初始化过程中，但是在运行梯度下降时， w和b变得更大。可能只有在w和b接近0时，反向传播的实施才是正确的。但当w和b变大时，它变得越来越不准确。可以在随机初始化过程中，运行梯度检验，然后在训练网络，w和b会有一段时间远离0，如果随机初始化值比较小，反复训练网络之后，再重新运行梯度检验。  
### 超参数
学习率    根据cost函数J是否下降并且收敛值进行比较 做为标准  
梯度下降循环数  
隐层数   
隐层单元数  
激活函数  
momentun   
mini batchsize  
regularization parameters 正则化参数

## 优化算法
提高训练速度
### mini-batch梯度下降法
将大的训练家拆分成多个小训练集用X{t} Y{t}表示，并用小的集，分别训练，优化w和b。在mini-batch时，一次处理进行batchsize个梯度下降。mini-batch下降曲线会出现噪音等情况。batchsize一般会取2的幂值，如64、128、256、512。  
![](/assets/img/tensorflow/data/deeplearning/28.jpg)  

### 指数加权平均
$V_t = \beta V_{t-1} + (1-\beta ) \theta_t $  
以一种简便快速易应用的方法求平均。求$\frac{1}{1-\beta}$ 个数值的均值。由于当$\frac{1}{1-\beta}$以外的数据其权重都约等于 $\frac{1}{e}$ 可以进行忽略  
#### 偏差修正
由于刚开始进行计算时初始化V的值为0，则会产生误差。可以使用$V_t = \frac{V_t}{1-\beta^t}$ ，进行偏差修正。当t越来越大是将作用越来越小，符合数据的情况。  

### Momentum动量梯度下降法
该方法速度较快，计算梯度的指数加权平均数，并利用该梯度更新你的权重。  
比较适用于切面为椭圆形的等值分布上。会降低纵轴的摆动，加快横轴的速度。  
$V_{dw} = \betaV_{dw} + (1-\beta)dW $  
$V_{db} = \betaV_{db} + (1-\beta)db $  
$W = W - \alpha V_{dw} , b = b - \aplpha V_{db}$  

### RMSprop
优化公式  
\[S_{dw} = \beta_2 S_{dw} + (1-\beta_2)dw^2 \]  
$$S_{db} = \beta_2 S_{db} + (1-\beta_2)db^2 $$  
\(w = w - \aplpha\frac{dw}{\sqrt{S_dw}+\epsilon}\)  

### Adam (Adaptive Moment Estimation)
$V_{dw} = \beta_1V_{dw} + (1-\beta_1)dW $   
$V_{db} = \beta_1V_{db} + (1-\beta_1)db $  
$S_{dw} = \beta_2 S_{dw} + (1-\beta_2)dw^2 $  
$S_{db} = \beta_2 S_{db} + (1-\beta_2)db^2 $  

$V_{dw} = \frac{V_{dw}}{1-\beta_1^t}$  
$V_{db} = \frac{V_{db}}{1-\beta_1^t}$  
$S_{dw} = \frac{S_{dw}}{1-\beta_2^t}$  
$S_{db} = \frac{S_{db}}{1-\beta_2^t}$  
$w = w - \aplpha\frac{V_{dw}}{\sqrt{S_dw}+\epsilon}$  
$b = b - \aplpha\frac{V_{db}}{\sqrt{S_db}+\epsilon}$  

$\aplpha$ 需要不断的调试
$\beta_1$ 一般为0.9   
$\beta_2$ 一般为0.999  
$\epsilon$ 一般为$10^{-8}$  

### 学习率衰减
能够降低梯度下降在最低点的摆动。  
epoch-num为mini-batch梯度下降代数。  
decay-rate为衰减率 超参数。  
k为常量 超参数。  
1、$\aplpha = \frac{1}{1+decay-rate * epoch-num} * \aplpha$    
2、$\aplpha = 0.95^{epoch-num} * \aplpha $    
3、$\aplpha = \frac{k}{\sqrt{epoch-num}} * \aplpha $    
4、离散衰减

### 局部最优的问题

需要考虑的是鞍点，在高维度时，当所有的维度呈现的是凹函数是才是局部最优点，我们经常遇到的应该是凹函数凸函数都存在的点，这就是鞍点。需要让其尽快的从鞍点的平缓区走出来继续进行优化。  

## 超参数调试 Batch正则化和程序框架

### 调试处理
![](/assets/img/tensorflow/data/deeplearning/30.jpg)
随机点取值比网格点取值，可以尝试更多的参数值，更适合调试。  
可以在最好的点区域，在进行密集取值，找到更好的超参数值。（先粗糙后细致）  

### 为超参数选择适合的范围
$\frac{1}{1-\beta}$
中当$\beta$ 越趋近1时，灵敏度会越高。平均的参数样本越来越多。  
先将数轴进行分段，均匀选择参数进行。  

![](/assets/img/tensorflow/data/deeplearning/31.jpg)
以不断变换r的值，进行学习率的取值。  

### 超参数训练的实践
1、超参数可能会通过跨域获得更好的灵感，以及适用性。    
2、超参数因为数据的不断变更，在几个月就要更新一下超参数，以满足训练。  
3、取决于计算资源可以选择熊猫式培养和鱼子酱式培养。  

### 归一化网络的激活函数
batch归一化，将隐藏层的值a在激活函数之前进行归一化，提高训练速度。在正向传播应用，反向传播调整参数。
公式：
$\mu = \frac{1}{m}\sumZ^{i}$  
$\sigma^2 = \frac{1}{m}\sum(Z^{i}-\mu)^2$
$Z^i_norm = \frac{z^i-\mu}{\sqrt{\sigma^2+\epsilon}}$  
$Z^{Ni} = \vartheta Z^i_norm +\beta $

由于进行batch归一化，那么原本z=wa+b ，去均值除方差，那么b就没有意义了，所以可以将z=wa。  
batch归一化中，为了不让a值归一化后均值为0，方差为1(如果使用sigmod激活函数，通常不希望值范围在线性部分上)。所以需要调整$\vartheta$ 、$\beta$ 可以将Z取值范围改为任意。  
batch归一化通常和mini-batch一起使用。  

应用：tf.nn.batch_normalization

作用：降低了神经网络层的互相的强依赖，尽可能提升独立性。防止因训练数据的独特性导致神经网络失效。  
噪音：由于batch归一化结合mini-batch所以局部数据、缩放(batch归一化)过程都会产生噪音。与实际样本数据。  
（dropout同样具有噪音（0或1），正则化作用）  
由于噪音batch具有轻微正则化的作用，可以根据batch-size的大小，决定了正则化的轻重。

应用场景：测试集使用batch归一化时。1、使用训练集时的值。2、运用指数加权平均来追踪。3、流动平均来粗略估算。

### Softmax 激活函数

逻辑回归，根据倒数第二层的输出，进行e的幂函数，之后归一化（$a^l = \frac{t_i}{\sum t_i}$），获得分属于不同类别的概率（概率和为1）。  

应用：损失函数 $L(y^{^},y) = - \sum y_jlogy^{^}_j$  (j=[1,4])
