---
layout: post
title: '深度学习-基础知识'
date: 2018-01-22
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 深度学习 神经网络
---
> 网易云课堂-吴恩达深度学习视频知识整理

## 深度学习
```python
There is been a lot of hype(炒作) about neural networks.
```
### 神经网络的应用范围
价格预测 图像学习 推荐系统(recommender systems) 音频 文本

### 神经网络的类型

![](/assets/img/tensorflow/data/deeplearning/1.jpg)

房产、推荐系统  标准的神经网络
图像领域 经常使用卷积神经网络 convolutional neural networks
音频、语言(序列文件包含时间的概念)  循环神经网络 recurrent neural networks
复杂问题 custom complex hybrid neural networks 混合神经网络

### structured data(结构化数据) and unstructured data(非结构化数据)

structured data 多为数据库中的数据，指每个输入的变量，都有着清晰的定义
unstructured data 例如声音图像等需要进行分析像素值，为非结构化数据

用结构化的数据做为分析，主要依托着海量的数据库数据，进行趋势的预测

### neural networks 为什么近期发展起来

![](/assets/img/tensorflow/data/deeplearning/2.jpg)

数据的规模(m 标识训练样本的数据规模)
神经网络的规模
计算能力的规模

### 函数
sigmod function
relu  
用激活函数sigmod时，在函数的上界下界时，梯度接近0，学习的速度会非常缓慢。通过改变激活函数

### 二分分类

对于训练数据，不需要进行for遍历的形式进行分析数据

正向传播

反向传播

图片，彩色图片的颜色通道数为3，黑白图片的颜色通道数为1。用特征向量进行表示全部的像素点，维度为n*n*通道数

![logistic回归及神经网络的符号](/assets/img/tensorflow/data/deeplearning/4.jpg)

#### logistic回归

logistic回归适用于二分分类的算法。

假设函数 wx+b 是返回的值是没有界限的，不满足二分的要求，所以存在sigmod函数或者refu函数将其转换成二分结果

![](/assets/img/tensorflow/data/deeplearning/5.jpg)

#### 损失函数(Loss) 对于单个训练样本的表现
其中如果选择1/2(y^-y)的平方，会出现非凸函数，无法求的全局最优解
需要选择L(y^,y)=-(ylog(y^)+(1-y)log(1-y^))                  e为底   导数为 -y/y^+(1-y)/(1-y^)
![](/assets/img/tensorflow/data/deeplearning/6.jpg)
需要让误差的平方越小越好，这个L(y^,y)=-(ylog(y^)+(1-y)log(1-y^))也是越小越好
y^的取值范围是0~1
当y=1时，L(y^,y)=-log(y^) 想要log(y^)越大 y^越大
当y=0时，L(y^,y)=-log(1-y^) 想要log(1-y^)越大 y^越小   
![](/assets/img/tensorflow/data/deeplearning/7.jpg)

梯度下降更新w,b值
![](/assets/img/tensorflow/data/deeplearning/8.jpg)

logistic回归反向传播，计算导数，链式法则
![](/assets/img/tensorflow/data/deeplearning/9.jpg)
loss函数推导（??负号的添加 ）
![](/assets/img/tensorflow/data/deeplearning/11.jpg)
#### 成本函数(cost) 对于全体训练样本的表现
J(w,b) = 1/m*(L(y^(i),y(i))的1到m的累和)

cost函数推导（??负号的删除 ）
![](/assets/img/tensorflow/data/deeplearning/12.jpg)
cost做梯度下降
![](/assets/img/tensorflow/data/deeplearning/10.jpg)
计算每个训练样本对应w1 w2 b的导数，做累加并且最后取平均，做为梯度下降公式的导数参数，再根据例如w1= w1-学习率*w1对应的导数 进行调整w1

导数公式
ax  导数 为 a
x(n)   导数为nx(n-1)
ln(x))  导数为  1/x

#### 向量化
避免显式for循环，使用向量化操作，并行计算
#### SIMD
SIMD单指令流多数据流(SingleInstruction Multiple Data,SIMD)是一种采用一个控制器来控制多个处理器，同时对一组数据（又称“数据向量”）中的每一个分别执行相同的操作从而实现空间上的并行性的技术。
