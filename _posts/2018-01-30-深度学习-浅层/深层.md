---
layout: post
title: '深度学习-浅层/深层'
date: 2018-01-22
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 深度学习 浅层/深层神经网络
---
> 网易云课堂-吴恩达深度学习视频知识整理

## 浅层神经网络

向量化处理多个样本多个特征的神经网络计算。根据将X变量进行列放大。

tanh(z)函数 优于 sigmod(z)函数，数据中心化，激活函数均值更接近0。
sigmod函数可以用于二分问题的输出层。
tanh(z)函数和sigmod函数都具有到达最大值最小值时，梯度很小接近0，拖慢梯度下降
Relu函数  该函数在0点斜率无意义，程序中很难发生

![](/assets/img/tensorflow/data/deeplearning/13.jpg)

### 非线性的函数(激活函数必要性)
如果才采用恒等的激活函数，会使最后输出的值一样为线性函数，这样多少隐藏层都没有存在的意义。
线性激活函数在回归问题的输出项会派上用场，隐藏层也需要非线性的激活函数。

![](/assets/img/tensorflow/data/deeplearning/14.jpg)

### 激活函数的导数

![](/assets/img/tensorflow/data/deeplearning/15.jpg)
![](/assets/img/tensorflow/data/deeplearning/16.jpg)
![](/assets/img/tensorflow/data/deeplearning/17.jpg)
Relu和Leaky Relu函数可以用代码设置当为0时，其梯度，不会对结果产生较大影响。

### 前向传播、反向传播涉及的公式

![](/assets/img/tensorflow/data/deeplearning/18.jpg)

### 随机初始化

权重参数w，需要进行随机初始化，否则会出现隐藏层节点计算完全一样。w相同，输出相同，反向传播导数相同，每次梯度下降更改的数值也相同。

（单层神经网络）将权重参数w调节成非常小，缩放100倍，如果使用sigmod或者tanh函数会到达梯度下降速度平缓的区域。


## 深层神经网络

### 深层网络中的前向传播
步骤与浅层相同，重复n次

### 核对矩阵的维数
验证神经网络的维度是否正确
Z[l] = W[l] * X +b[l]
维度 （n[l],1）=(n[l],n[l-1]) * (n[l-1],1) +（n[l],1）
数据并行计算（向量化） 维度 （n[l],m）=(n[l],n[l-1]) * (n[l-1],m) +（n[l],1）广播
W维度即为(当前层节点数，上一层节点数) W[l] : (n[l],n[l-1])
dW维度与W维度相同，db维度与b维度相同

### 正向传播反向传播示意图
其中编程中将z  w  b  进行缓存有助于程序运行  
正向输入为X特征向量   反向输入为L(y^,y)的导数
![](/assets/img/tensorflow/data/deeplearning/19.jpg)

## 深层和浅层神经网络对比

1、深层神经网络每一层干着由浅入深的并且是不同的环节   (递进思维)
2、分层会大幅度减少层中的节点数，O(logn)和O（2的n次幂）  （电路思维）


## 参数及超参数
