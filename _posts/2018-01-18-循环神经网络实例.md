---
layout: post
title: '循环神经网络实例'
date: 2018-01-18
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 神经网络
---
> 循环神经网络

## 循环神经网络
batch 之间是有相互影响的关系。
数据格式：序列化的数据  如将图片28*28的像素点切分成28行 每行输入
每个RNN输出两项，一项用于做最终的out,另外一项用于做下一行RNN的部分输入
获得，最后一个行样本的输出值
![](/assets/img/tensorflow/data/rnn/1.jpg)


```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

# 读取MNIST数据
mnist = input_data.read_data_sets("data", one_hot = True)
trainimgs = mnist.train.images
trainlabels = mnist.train.labels
testimgs = mnist.test.images
testlabels = mnist.test.labels
ntrain, ntest, dim, nclasses = trainimgs.shape[0], testimgs.shape[0], trainimgs[1], trainlabels.shape[1]
print("MNIST READY!")
nclasses = 10
diminput = 28
dimhidden = 128
dimoutput = nclasses
# 分成多少步
nsteps = 28
weights = {
    # 高斯初始化
    "hidden" : tf.Variable(tf.random_normal([diminput, dimhidden])),
    "out" : tf.Variable(tf.random_normal([dimhidden, dimoutput]))
}

biases = {
    "hidden" : tf.Variable(tf.random_normal([dimhidden])),
    "out" : tf.Variable(tf.random_normal([dimoutput]))
}

def _RNN(_X, _W, _b ,_nsteps, _name):
    # 格式转换  Permute input  from [batchsize,nsteps.diminput]  to [nsteps,batchsize,diminput]
    _X = tf.transpose(_X, [1,0,2])
    # reshape input to [nsteps*batchsize, diminput]
    _X = tf.reshape(_X, [-1, diminput])
    #   矩阵相乘   输出的是一个batchsize整体
    _H = tf.matmul(_X, _W["hidden"]) + _b["hidden"]
    #  切分
    _Hsplit = tf.split(_H, _nsteps, 0)
    # 给变量定义名字
    with tf.variable_scope(_name):
        # , reuse=tf.AUTO_REUSE
        # 变量共享操作      多次创建是一个东西
        #   命名域 _name/w                                   第一次不忘记
        lstm_cell = tf.contrib.rnn.BasicLSTMCell(dimhidden, forget_bias=1.0)
        _LSTM_O, _LSTM_S = tf.contrib.rnn.static_rnn(lstm_cell, _Hsplit, dtype=tf.float32)
    #              _LSTM_O最后一个
    _O = tf.matmul(_LSTM_O[-1], _W["out"]) + _b["out"]

    return {
        "X":_X, "H":_H,"Hsplit":_Hsplit,"LSTM_O":_LSTM_O,"LSTM_S":_LSTM_S,"O":_O
    }

print("RNN READY!")

learning_rate = 0.001
x = tf.placeholder("float", [None, nsteps, diminput])
y = tf.placeholder("float", [None, dimoutput])
myrnn = _RNN(x, weights, biases, nsteps, "basic")
pred = myrnn["O"]
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y, logits=pred))
optm = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
accr = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1),tf.argmax(y, 1)), tf.float32))


print("Network Ready!")

training_epochs = 5
batch_size = 16
display_step = 1

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for epoch in range(training_epochs):
        avg_cost = 0.
#        total_batch = int(mnist.train.num_examples/batch_size)
        total_batch = 100
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            batch_xs = batch_xs.reshape((batch_size, nsteps, diminput))
            sess.run(optm, feed_dict={x: batch_xs, y:batch_ys})
            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y:batch_ys})/total_batch
        if epoch % display_step == 0:
            print ("Epoch: %03d/%03d cost: %.9f" % (epoch+1, training_epochs, avg_cost))
            feeds = {x: trainimgs.reshape((ntrain, nsteps, diminput)), y: trainlabels}
            train_acc = sess.run(accr, feed_dict=feeds)
            print (" Training accuracy: %.3f" % (train_acc))
            feeds = {x: testimgs.reshape((ntest, nsteps, diminput)), y: testlabels}
            test_acc = sess.run(accr, feed_dict=feeds)
            print (" Test accuracy: %.3f" % (test_acc))

print ("FINISHED")







```
