---
layout: post
title: '深度学习-浅层'
date: 2018-01-22
author: MrXuyc
categories: 技术
cover: '/assets/img/tensorflow/tensorflow1.jpg'
tags: 深度学习 浅层神经网络
---
> 网易云课堂-吴恩达深度学习视频知识整理

## 浅层神经网络

向量化处理多个样本多个特征的神经网络计算。根据将X变量进行列放大。

tanh(z)函数 优于 sigmod(z)函数，数据中心化，激活函数均值更接近0。
sigmod函数可以用于二分问题的输出层。
tanh(z)函数和sigmod函数都具有到达最大值最小值时，梯度很小接近0，拖慢梯度下降
Relu函数  该函数在0点斜率无意义，程序中很难发生

![](/assets/img/tensorflow/data/deeplearning/13.jpg)

### 非线性的函数(激活函数必要性)
如果才采用恒等的激活函数，会使最后输出的值一样为线性函数，这样多少隐藏层都没有存在的意义。
线性激活函数在回归问题的输出项会派上用场，隐藏层也需要非线性的激活函数。

![](/assets/img/tensorflow/data/deeplearning/14.jpg)

### 激活函数的导数

![](/assets/img/tensorflow/data/deeplearning/15.jpg)
![](/assets/img/tensorflow/data/deeplearning/16.jpg)
![](/assets/img/tensorflow/data/deeplearning/17.jpg)
Relu和Leaky Relu函数可以用代码设置当为0时，其梯度，不会对结果产生较大影响。

### 前向传播、反向传播涉及的公式

![](/assets/img/tensorflow/data/deeplearning/18.jpg)

### 随机初始化

权重参数w，需要进行随机初始化，否则会出现隐藏层节点计算完全一样。w相同，输出相同，反向传播导数相同，每次梯度下降更改的数值也相同。

（单层神经网络）将权重参数w调节成非常小，缩放100倍，如果使用sigmod或者tanh函数会到达梯度下降速度平缓的区域。
